import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from bs4 import BeautifulSoup
import time

# --- 1. åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰", layout="wide")

# ã€é‡è¤‡å›é¿ç”¨ã€‘åˆ—åãŒé‡ãªã£ãŸå ´åˆã«è‡ªå‹•ã§ç•ªå·ã‚’æŒ¯ã‚‹é–¢æ•°
def make_columns_unique(df):
    cols = []
    counts = {}
    for col in df.columns:
        col_str = str(col).strip()
        if col_str in counts:
            counts[col_str] += 1
            cols.append(f"{col_str}_{counts[col_str]}")
        else:
            counts[col_str] = 0
            cols.append(col_str)
    df.columns = cols
    return df

def to_half_width(text):
    if pd.isna(text): return text
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    return re.sub(r'[^\d\.]', '', str(text).translate(table))

def normalize_name(x):
    if pd.isna(x): return ''
    s = str(x).strip().replace('ã€€', '').replace(' ', '')
    return re.split(r'[,(ï¼ˆ/]', s)[0]

JYO_MAP = {'01':'æœ­å¹Œ','02':'å‡½é¤¨','03':'ç¦å³¶','04':'æ–°æ½Ÿ','05':'æ±äº¬','06':'ä¸­å±±','07':'ä¸­äº¬','08':'äº¬éƒ½','09':'é˜ªç¥','10':'å°å€‰'}

# --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆé‡è¤‡å¯¾ç­–ç‰ˆï¼‰ ---
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        # èª­ã¿è¾¼ã¿ç›´å¾Œã«é‡è¤‡ã‚’è§£æ¶ˆ
        df = make_columns_unique(df)

        # é …ç›®åã‚’æ¢ã™ï¼ˆ20è¡Œç›®ã¾ã§ã‚¹ã‚­ãƒ£ãƒ³ï¼‰
        for i in range(min(len(df), 20)):
            row_vals = [str(x) for x in df.iloc[i].values]
            if any('å ´æ‰€' in x or 'R' in x or 'é¦¬å' in x for x in row_vals):
                df.columns = df.iloc[i]
                df = df.iloc[i+1:].reset_index(drop=True)
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šå¾Œã«ã‚‚ã†ä¸€åº¦é‡è¤‡ã‚’è§£æ¶ˆ
                df = make_columns_unique(df)
                break
        
        # åˆ—åã®åå¯„ã›
        name_map = {'å ´æ‰€':'å ´å','R':'R','ï¼²':'R','ç•ª':'æ­£ç•ª','é¦¬ç•ª':'æ­£ç•ª','ç€é †':'ç€é †','ç€':'ç€é †','å˜å‹ã‚ªãƒƒã‚º':'å˜ï½µï½¯ï½½ï¾','ã‚ªãƒƒã‚º':'å˜ï½µï½¯ï½½ï¾'}
        new_cols = []
        for c in df.columns:
            target = str(c).strip()
            for k, v in name_map.items():
                if k in target:
                    target = v
                    break
            new_cols.append(target)
        
        df.columns = new_cols
        # åå¯„ã›å¾Œï¼ˆã€Œå ´æ‰€ã€ã¨ã€Œä¼šå ´ã€ãŒä¸¡æ–¹ã€Œå ´åã€ã«ãªã£ãŸå ´åˆãªã©ï¼‰ã«å†åº¦é‡è¤‡ã‚’è§£æ¶ˆ
        df = make_columns_unique(df)

        # å¿…é ˆåˆ—ã®ç¢ºä¿
        for col in ['å ´å', 'R', 'æ­£ç•ª', 'ç€é †']:
            if col not in df.columns: df[col] = np.nan
        
        return df, "success"
    except Exception as e:
        return pd.DataFrame(), str(e)

# --- 3. ãƒãƒƒãƒˆç«¶é¦¬ãƒ‡ãƒ¼ã‚¿å–å¾— ---
def fetch_netkeiba_result(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'EUC-JP'
        
        rid_match = re.search(r'race_id=(\d{12})', url)
        info = {"place": JYO_MAP.get(rid_match.group(1)[4:6], "") if rid_match else "", "r": int(rid_match.group(1)[10:12]) if rid_match else 0}

        soup = BeautifulSoup(response.text, 'html.parser')
        table = soup.find('table', id='All_Result_Table') or soup.find('table', class_=lambda x: x and 'ResultRefund' in x)
        if not table: return None, info, "è¡¨ãªã—"

        result_map = {}
        rows = table.find_all('tr', class_=lambda x: x and 'HorseList' in x)
        for row in rows:
            cols = row.find_all('td')
            if len(cols) < 3: continue
            r_m = re.search(r'\d+', cols[0].get_text(strip=True))
            u_m = re.search(r'\d+', cols[2].get_text(strip=True))
            if u_m: result_map[int(u_m.group())] = int(r_m.group()) if r_m else 99
        return result_map, info, "success"
    except Exception as e: return None, None, str(e)

# --- 4. UI ç”»é¢ ---
st.title("ğŸ‡ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé‡è¤‡ã‚¨ãƒ©ãƒ¼å¯¾ç­–ç‰ˆï¼‰")

with st.sidebar:
    st.header("ğŸ“‚ 1. ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ")
    up_curr = st.file_uploader("å½“æ—¥é…ç½®è¡¨(Excel/CSV)", type=['xlsx', 'csv'])
    
    if 'df' in st.session_state:
        st.divider()
        st.header("ğŸ’¾ 3. ä¿å­˜")
        csv = st.session_state['df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv, "horse_results.csv")
        if st.button("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢"):
            del st.session_state['df']
            st.rerun()

if up_curr:
    if 'df' not in st.session_state:
        df, status = load_data(up_curr)
        if status == "success":
            st.session_state['df'] = df
        else:
            st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {status}")

    if 'df' in st.session_state:
        st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        
        # URLè²¼ã‚Šä»˜ã‘ã‚¨ãƒªã‚¢
        st.header("ğŸ”— 2. URLä¸€æ‹¬è²¼ã‚Šä»˜ã‘")
        urls_input = st.text_area("ãƒãƒƒãƒˆç«¶é¦¬çµæœURLï¼ˆ1è¡Œã«1ã¤ãšã¤ï¼‰", height=200)
        
        if st.button("ğŸš€ ä¸€æ‹¬å–å¾—é–‹å§‹"):
            if urls_input:
                urls = [u.strip() for u in urls_input.split('\n') if u.strip()]
                progress = st.progress(0)
                status_box = st.empty()
                
                for i, url in enumerate(urls):
                    status_box.text(f"å‡¦ç†ä¸­ ({i+1}/{len(urls)}): {url[-12:]}")
                    res, info, msg = fetch_netkeiba_result(url)
                    if msg == "success":
                        for u, r in res.items():
                            st.session_state['df'].loc[
                                (st.session_state['df']['å ´å']==info['place']) & 
                                (st.session_state['df']['R']==info['r']) & 
                                (st.session_state['df']['æ­£ç•ª']==u), 'ç€é †'
                            ] = r
                    progress.progress((i+1)/len(urls))
                    time.sleep(1)
                
                status_box.success("å…¨ã¦ã®å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                st.rerun()

        st.divider()
        st.subheader("ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³")
        # è¡¨ç¤ºç›´å‰ã«å¿µã®ãŸã‚åˆ—åã®é‡è¤‡ãŒãªã„ã‹å†åº¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        final_df = make_columns_unique(st.session_state['df'].copy())
        st.dataframe(final_df, use_container_width=True)

else:
    st.info("ğŸ‘ˆ å·¦å´ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¾ã›ã¦ãã ã•ã„ã€‚")
