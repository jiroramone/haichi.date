import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from bs4 import BeautifulSoup
import time

# --- 1. åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸ ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆä¸€æ‹¬å–å¾—ç‰ˆï¼‰", layout="wide")

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    return re.sub(r'[^\d\.]', '', text.translate(table))

def normalize_name(x):
    if pd.isna(x): return ''
    s = str(x).strip().replace('ã€€', '').replace(' ', '')
    s = re.split(r'[,(ï¼ˆ/]', s)[0]
    return re.sub(r'[â˜…â˜†â–²â–³â—‡$*]', '', s)

JYO_MAP = {
    '01': 'æœ­å¹Œ', '02': 'å‡½é¤¨', '03': 'ç¦å³¶', '04': 'æ–°æ½Ÿ', '05': 'æ±äº¬',
    '06': 'ä¸­å±±', '07': 'ä¸­äº¬', '08': 'äº¬éƒ½', '09': 'é˜ªç¥', '10': 'å°å€‰'
}

# --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå¤±æ•—æ™‚ã«è©³ç´°ã‚’å‡ºã™ã‚ˆã†ã«å¼·åŒ–ï¼‰ ---
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        # é …ç›®åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€10è¡Œç›®ã¾ã§ã‚¹ã‚­ãƒ£ãƒ³
        if not any(col in str(df.columns) for col in ['å ´æ‰€', 'é¦¬', 'ç•ª', 'R']):
            for i in range(min(len(df), 10)):
                row_values = [str(x) for x in df.iloc[i].values]
                if any('å ´æ‰€' in x or 'ç•ª' in x or 'R' in x for x in row_values):
                    df.columns = df.iloc[i]
                    df = df.iloc[i+1:].reset_index(drop=True)
                    break

        df.columns = df.columns.astype(str).str.strip()
        name_map = {'å ´æ‰€':'å ´å','ç«¶é¦¬å ´':'å ´å','é–‹å‚¬':'å ´å','ç•ª':'æ­£ç•ª','é¦¬ç•ª':'æ­£ç•ª','å˜å‹ã‚ªãƒƒã‚º':'å˜ï½µï½¯ï½½ï¾','ã‚ªãƒƒã‚º':'å˜ï½µï½¯ï½½ï¾','ç€':'ç€é †'}
        df = df.rename(columns=name_map)
        
        # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
        missing = [c for c in ['R', 'å ´å', 'æ­£ç•ª'] if c not in df.columns]
        if missing:
            return pd.DataFrame(), f"ä¸è¶³ã—ã¦ã„ã‚‹åˆ—ãŒã‚ã‚Šã¾ã™: {', '.join(missing)}"

        df['R'] = pd.to_numeric(df['R'].apply(to_half_width), errors='coerce')
        df['æ­£ç•ª'] = pd.to_numeric(df['æ­£ç•ª'].apply(to_half_width), errors='coerce')
        df = df.dropna(subset=['R', 'æ­£ç•ª'])
        df['R'] = df['R'].astype(int)
        df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)
        for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'é¦¬å', 'å ´å']:
            if col in df.columns:
                df[col] = df[col].astype(str).apply(normalize_name)
        return df.copy(), "success"
    except Exception as e:
        return pd.DataFrame(), str(e)

# --- 3. ãƒãƒƒãƒˆç«¶é¦¬ãƒ‡ãƒ¼ã‚¿å–å¾— ---
def fetch_netkeiba_result(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'EUC-JP'
        if response.status_code != 200: return None, None, f"ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦({response.status_code})"

        race_id_match = re.search(r'race_id=(\d{12})', url)
        info = {"place": "", "r": 0}
        if race_id_match:
            rid = race_id_match.group(1)
            info["place"] = JYO_MAP.get(rid[4:6], "")
            info["r"] = int(rid[10:12])

        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.find_all('table')
        target_table = None
        for t in tables:
            t_text = t.get_text()
            if 'ç€é †' in t_text and 'é¦¬ç•ª' in t_text:
                target_table = t; break
        
        if not target_table: return None, info, "çµæœè¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

        result_map = {}
        rows = target_table.find_all('tr')
        header_cols = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]
        try:
            idx_rank = [i for i, c in enumerate(header_cols) if 'ç€é †' in c][0]
            idx_umaban = [i for i, c in enumerate(header_cols) if 'é¦¬ç•ª' in c][0]
        except: return None, info, "åˆ—ç‰¹å®šå¤±æ•—"

        for row in rows[1:]:
            cols = row.find_all('td')
            if len(cols) <= max(idx_rank, idx_umaban): continue
            r_m = re.search(r'\d+', cols[idx_rank].get_text(strip=True))
            u_m = re.search(r'\d+', cols[idx_umaban].get_text(strip=True))
            if r_m and u_m: result_map[int(u_m.group())] = int(r_m.group())
            elif u_m: result_map[int(u_m.group())] = 99
        
        return result_map, info, "success"
    except Exception as e: return None, None, str(e)

# --- 4. UI ç”»é¢è¡¨ç¤º ---
st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬åé›†")

with st.sidebar:
    st.header("ğŸ“‚ 1. ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ")
    up_curr = st.file_uploader("ã„ã¤ã‚‚ã®é…ç½®è¡¨(Excel/CSV)ã‚’ã‚¢ãƒƒãƒ—", type=['xlsx', 'csv'])
    
    if 'analyzed_df' in st.session_state:
        st.divider()
        st.header("ğŸ’¾ 3. ä¿å­˜")
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ç€é †å…¥ã‚ŠCSVã‚’ä¿å­˜", csv, "horse_results.csv")
        if st.button("ğŸ—‘ï¸ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢"):
            del st.session_state['analyzed_df']
            st.rerun()

# ãƒ¡ã‚¤ãƒ³ç”»é¢ã®åˆ¶å¾¡
if up_curr:
    df_raw, status = load_data(up_curr)
    
    if status == "success":
        if 'analyzed_df' not in st.session_state:
            st.session_state['analyzed_df'] = df_raw
        
        # --- URLå…¥åŠ›ç”»é¢ã‚’è¡¨ç¤º ---
        st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸï¼")
        
        st.header("ğŸ”— 2. URLã‚’è²¼ã‚Šä»˜ã‘ã¦ç€é †ã‚’ä¸€æ‹¬å–å¾—")
        st.info("ãƒãƒƒãƒˆç«¶é¦¬ã®ãƒ¬ãƒ¼ã‚¹çµæœURLï¼ˆ...result.html?race_id=...ï¼‰ã‚’ä¸‹ã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")
        
        urls_input = st.text_area("ã“ã“ã«URLã‚’ã¾ã¨ã‚ã¦è²¼ã‚Šä»˜ã‘ï¼ˆ1è¡Œã«1ã¤ã®URLï¼‰", height=250)
        
        if st.button("ğŸš€ ä¸€æ‹¬å–å¾—ã‚’é–‹å§‹ã™ã‚‹"):
            if not urls_input:
                st.error("URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            else:
                urls = [u.strip() for u in urls_input.split('\n') if u.strip()]
                progress_bar = st.progress(0)
                status_box = st.empty()
                
                success_count = 0
                for i, url in enumerate(urls):
                    status_box.text(f"å–å¾—ä¸­ ({i+1}/{len(urls)}): {url[-12:]}")
                    res, info, msg = fetch_netkeiba_result(url)
                    
                    if msg == "success" and info["place"]:
                        # è©²å½“ã™ã‚‹ãƒ¬ãƒ¼ã‚¹ã®ç€é †ã‚’æ›´æ–°
                        st.session_state['analyzed_df'].loc[
                            (st.session_state['analyzed_df']['å ´å'] == info["place"]) & 
                            (st.session_state['analyzed_df']['R'] == info["r"]), 'ç€é †'
                        ] = np.nan # ä¸€æ—¦ã‚¯ãƒªã‚¢
                        
                        for u, r in res.items():
                            st.session_state['analyzed_df'].loc[
                                (st.session_state['analyzed_df']['å ´å'] == info["place"]) & 
                                (st.session_state['analyzed_df']['R'] == info["r"]) & 
                                (st.session_state['analyzed_df']['æ­£ç•ª'] == u), 'ç€é †'
                            ] = r
                        success_count += 1
                    else:
                        st.warning(f"å–å¾—å¤±æ•—: {url[-12:]} ({msg})")
                    
                    progress_bar.progress((i + 1) / len(urls))
                    time.sleep(1.2) # ãƒ–ãƒ­ãƒƒã‚¯é˜²æ­¢
                
                status_box.success(f"å®Œäº†ï¼ {len(urls)}ä»¶ä¸­ {success_count}ä»¶ã®ç€é †ã‚’åæ˜ ã—ã¾ã—ãŸã€‚")
                st.balloons()

        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
        st.divider()
        st.subheader("ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³")
        st.dataframe(st.session_state['analyzed_df'][['å ´å','R','æ­£ç•ª','é¦¬å','ç€é †','å˜ï½µï½¯ï½½ï¾']], height=400)

    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆã®åŸå› ã‚’è¡¨ç¤º
        st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚\nåŸå› : {status}")
        st.write("ã€Œå ´æ‰€ã€ã€ŒRã€ã€Œç•ªã€ã¨ã„ã†é …ç›®ãŒ1è¡Œç›®ã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.write("èª­ã¿è¾¼ã‚“ã ç”Ÿãƒ‡ãƒ¼ã‚¿:")
        st.write(df_raw) # ã©ã“ã¾ã§èª­ã¿è¾¼ã‚ãŸã‹è¡¨ç¤º
else:
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã¾ã ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„æ™‚
    st.info("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€ã„ã¤ã‚‚ã®é…ç½®è¡¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.image("https://raw.githubusercontent.com/streamlit/docs/main/public/images/tutorials/file-uploader.png", width=300) # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å‚è€ƒç”»åƒ
