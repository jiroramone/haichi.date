import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from bs4 import BeautifulSoup
import time

# --- 1. åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸ ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰", layout="wide")

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    return re.sub(r'[^\d\.]', '', text.translate(table))

def normalize_name(x):
    if pd.isna(x): return ''
    s = str(x).strip().replace('ã€€', '').replace(' ', '')
    s = re.split(r'[,(ï¼ˆ/]', s)[0]
    return re.sub(r'[â˜…â˜†â–²â–³â—‡$*]', '', s)

JYO_MAP = {
    '01': 'æœ­å¹Œ', '02': 'å‡½é¤¨', '03': 'ç¦å³¶', '04': 'æ–°æ½Ÿ', '05': 'æ±äº¬',
    '06': 'ä¸­å±±', '07': 'ä¸­äº¬', '08': 'äº¬éƒ½', '09': 'é˜ªç¥', '10': 'å°å€‰'
}

# --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆè¶…å¼·åŒ–ç‰ˆï¼‰ ---
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        # --- ãƒ˜ãƒƒãƒ€ãƒ¼è‡ªå‹•æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ ---
        # ã©ã®è¡Œã«ã€Œå ´æ‰€ã€ã‚„ã€ŒRã€ãªã©ã®é‡è¦ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹æ¢ã™
        header_row_index = 0
        found = False
        for i in range(min(len(df), 20)):
            row_vals = [str(x) for x in df.iloc[i].values]
            # ã€Œå ´æ‰€ã€ã‹ã€Œé¦¬åã€ãŒã‚ã‚Œã°ãã“ãŒãƒ˜ãƒƒãƒ€ãƒ¼
            if any('å ´æ‰€' in x or 'é¦¬å' in x or 'é¨æ‰‹' in x for x in row_vals):
                df.columns = df.iloc[i]
                df = df.iloc[i+1:].reset_index(drop=True)
                found = True
                header_row_index = i
                break
        
        df.columns = df.columns.astype(str).str.strip()
        
        # åˆ—åå¤‰æ›ãƒãƒƒãƒ—
        name_map = {
            'å ´æ‰€': 'å ´å', 'å ´å': 'å ´å', 'ç«¶é¦¬å ´': 'å ´å', 'é–‹å‚¬': 'å ´å',
            'R': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹': 'R',
            'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª', 'æ­£ç•ª': 'æ­£ç•ª',
            'ç€': 'ç€é †', 'ç€é †': 'ç€é †',
            'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾'
        }
        # æ—¢å­˜ã®åˆ—åã‹ã‚‰éƒ¨åˆ†ä¸€è‡´ã§æ¢ã—ã¦ç½®æ›
        new_cols = {}
        for col in df.columns:
            for k, v in name_map.items():
                if k in col:
                    new_cols[col] = v
                    break
        df = df.rename(columns=new_cols)

        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«èª­ã¿è¾¼ã‚“ã ç›´å¾Œã®çŠ¶æ…‹ã‚’ä¿æŒ
        raw_cols = df.columns.tolist()

        # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆç·©ã‚ã‚‹ï¼‰
        if 'R' not in df.columns or 'å ´å' not in df.columns or 'æ­£ç•ª' not in df.columns:
            return df, f"å¿…é ˆé …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è¦‹ã¤ã‹ã£ãŸé …ç›®: {raw_cols}"

        # ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        df['R'] = pd.to_numeric(df['R'].apply(to_half_width), errors='coerce')
        df['æ­£ç•ª'] = pd.to_numeric(df['æ­£ç•ª'].apply(to_half_width), errors='coerce')
        df = df.dropna(subset=['R', 'æ­£ç•ª'])
        df['R'] = df['R'].astype(int)
        df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)
        
        for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'é¦¬å', 'å ´å']:
            if col in df.columns:
                df[col] = df[col].astype(str).apply(normalize_name)
        
        if 'ç€é †' not in df.columns:
            df['ç€é †'] = np.nan

        return df.copy(), "success"
    except Exception as e:
        return pd.DataFrame(), str(e)

# --- 3. ãƒãƒƒãƒˆç«¶é¦¬å–å¾— (å¤‰æ›´ãªã—) ---
def fetch_netkeiba_result(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'EUC-JP'
        if response.status_code != 200: return None, None, f"æ‹’å¦({response.status_code})"

        race_id_match = re.search(r'race_id=(\d{12})', url)
        info = {"place": "", "r": 0}
        if race_id_match:
            rid = race_id_match.group(1)
            info["place"] = JYO_MAP.get(rid[4:6], "")
            info["r"] = int(rid[10:12])

        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.find_all('table')
        target_table = None
        for t in tables:
            t_text = t.get_text()
            if 'ç€é †' in t_text and 'é¦¬ç•ª' in t_text:
                target_table = t; break
        
        if not target_table: return None, info, "è¡¨ãªã—"

        result_map = {}
        rows = target_table.find_all('tr')
        header_cols = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]
        idx_rank = [i for i, c in enumerate(header_cols) if 'ç€é †' in c][0]
        idx_umaban = [i for i, c in enumerate(header_cols) if 'é¦¬ç•ª' in c][0]

        for row in rows[1:]:
            cols = row.find_all('td')
            if len(cols) <= max(idx_rank, idx_umaban): continue
            r_m = re.search(r'\d+', cols[idx_rank].get_text(strip=True))
            u_m = re.search(r'\d+', cols[idx_umaban].get_text(strip=True))
            if r_m and u_m: result_map[int(u_m.group())] = int(r_m.group())
            elif u_m: result_map[int(u_m.group())] = 99
        return result_map, info, "success"
    except Exception as e: return None, None, str(e)

# --- 4. UI ---
st.title("ğŸ‡ ãƒ‡ãƒ¼ã‚¿åé›†ãƒ‡ãƒãƒƒã‚°ç‰ˆ")

up_curr = st.sidebar.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])

if up_curr:
    df_raw, status = load_data(up_curr)
    
    if status == "success":
        st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«èªè­˜æˆåŠŸï¼")
        if 'analyzed_df' not in st.session_state:
            st.session_state['analyzed_df'] = df_raw
            
        # URLè²¼ã‚Šä»˜ã‘æ¬„
        st.header("ğŸ”— URLä¸€æ‹¬è²¼ã‚Šä»˜ã‘")
        urls_input = st.text_area("ã“ã“ã«1è¡Œãšã¤URLã‚’è²¼ã‚Šä»˜ã‘", height=200)
        if st.button("ä¸€æ‹¬å–å¾—é–‹å§‹"):
            urls = [u.strip() for u in urls_input.split('\n') if u.strip()]
            for i, url in enumerate(urls):
                st.write(f"å‡¦ç†ä¸­: {url[-12:]}")
                res, info, msg = fetch_netkeiba_result(url)
                if msg == "success":
                    for u, r in res.items():
                        st.session_state['analyzed_df'].loc[
                            (st.session_state['analyzed_df']['å ´å'] == info["place"]) & 
                            (st.session_state['analyzed_df']['R'] == info["r"]) & 
                            (st.session_state['analyzed_df']['æ­£ç•ª'] == u), 'ç€é †'] = r
                time.sleep(1)
            st.rerun()

        st.dataframe(st.session_state['analyzed_df'])
        
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.sidebar.download_button("ğŸ“¥ ä¿å­˜(CSV)", csv, "horse_results.csv")
        
    else:
        # å¤±æ•—ã—ãŸæ™‚ã®è©³ç´°è¡¨ç¤º
        st.error(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {status}")
        st.write("### ã‚ãªãŸã®ã‚¨ã‚¯ã‚»ãƒ«ã®çŠ¶æ…‹:")
        st.write("ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ã€å ´æ‰€ã€ã€Rã€ã€ç•ªã€ã¨ã„ã†åå‰ã®åˆ—ã‚’æ¢ã—ã¦ã„ã¾ã™ãŒã€è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.write("### å®Ÿéš›ã«èª­ã¿å–ã£ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®æ•°è¡Œï¼‰:")
        st.dataframe(df_raw.head(10)) # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ç¢ºèªã•ã›ã‚‹

else:
    st.info("å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
