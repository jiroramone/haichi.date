import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from bs4 import BeautifulSoup
import time

# --- 1. åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸ ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆä¸€æ‹¬å–å¾—ç‰ˆï¼‰", layout="wide")

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    return re.sub(r'[^\d\.]', '', text.translate(table))

def normalize_name(x):
    if pd.isna(x): return ''
    s = str(x).strip().replace('ã€€', '').replace(' ', '')
    s = re.split(r'[,(ï¼ˆ/]', s)[0]
    return re.sub(r'[â˜…â˜†â–²â–³â—‡$*]', '', s)

# ç«¶é¦¬å ´åã®å¤‰æ›ãƒãƒƒãƒ—ï¼ˆURLå†…ã®IDã‹ã‚‰åˆ¤å®šç”¨ï¼‰
JYO_MAP = {
    '01': 'æœ­å¹Œ', '02': 'å‡½é¤¨', '03': 'ç¦å³¶', '04': 'æ–°æ½Ÿ', '05': 'æ±äº¬',
    '06': 'ä¸­å±±', '07': 'ä¸­äº¬', '08': 'äº¬éƒ½', '09': 'é˜ªç¥', '10': 'å°å€‰'
}

# --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
@st.cache_data
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æ¢ç´¢
        if not any(col in str(df.columns) for col in ['å ´æ‰€', 'é¦¬', 'ç•ª', 'R']):
            for i in range(min(len(df), 10)):
                row_values = [str(x) for x in df.iloc[i].values]
                if any('å ´æ‰€' in x or 'ç•ª' in x or 'R' in x for x in row_values):
                    df.columns = df.iloc[i]; df = df.iloc[i+1:].reset_index(drop=True); break

        df.columns = df.columns.astype(str).str.strip()
        name_map = {'å ´æ‰€':'å ´å','ç«¶é¦¬å ´':'å ´å','é–‹å‚¬':'å ´å','ç•ª':'æ­£ç•ª','é¦¬ç•ª':'æ­£ç•ª','å˜å‹ã‚ªãƒƒã‚º':'å˜ï½µï½¯ï½½ï¾','ã‚ªãƒƒã‚º':'å˜ï½µï½¯ï½½ï¾','ç€':'ç€é †'}
        df = df.rename(columns=name_map)
        
        ensure_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'ç€é †']
        for col in ensure_cols:
            if col not in df.columns: df[col] = np.nan

        df['R'] = pd.to_numeric(df['R'].apply(to_half_width), errors='coerce')
        df['æ­£ç•ª'] = pd.to_numeric(df['æ­£ç•ª'].apply(to_half_width), errors='coerce')
        df = df.dropna(subset=['R', 'æ­£ç•ª'])
        df['R'] = df['R'].astype(int); df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)
        for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'é¦¬å', 'å ´å']:
            df[col] = df[col].astype(str).apply(normalize_name)
        df['å˜ï½µï½¯ï½½ï¾'] = pd.to_numeric(df['å˜ï½µï½¯ï½½ï¾'].apply(to_half_width), errors='coerce')
        return df.copy(), "success"
    except Exception as e: return pd.DataFrame(), str(e)

# --- 3. ãƒãƒƒãƒˆç«¶é¦¬ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚³ã‚¢æ©Ÿèƒ½ ---
def fetch_netkeiba_result(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'EUC-JP'
        if response.status_code != 200: return None, None, f"æ‹’å¦({response.status_code})"

        # URLã‹ã‚‰å ´åã¨Rã‚’è§£æ (ä¾‹: race_id=2025 07 0506 01)
        # 07=ä¸­äº¬, 01=1R
        race_id_match = re.search(r'race_id=(\d{12})', url)
        info = {"place": "", "r": 0}
        if race_id_match:
            rid = race_id_match.group(1)
            info["place"] = JYO_MAP.get(rid[4:6], "")
            info["r"] = int(rid[10:12])

        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.find_all('table')
        target_table = None
        for t in tables:
            t_text = t.get_text()
            if 'ç€é †' in t_text and 'é¦¬ç•ª' in t_text:
                target_table = t; break
        
        if not target_table: return None, info, "è¡¨ãªã—"

        result_map = {}
        rows = target_table.find_all('tr')
        header_cols = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]
        try:
            idx_rank = [i for i, c in enumerate(header_cols) if 'ç€é †' in c][0]
            idx_umaban = [i for i, c in enumerate(header_cols) if 'é¦¬ç•ª' in c][0]
        except: return None, info, "åˆ—ä¸æ˜"

        for row in rows[1:]:
            cols = row.find_all('td')
            if len(cols) <= max(idx_rank, idx_umaban): continue
            r_m = re.search(r'\d+', cols[idx_rank].get_text(strip=True))
            u_m = re.search(r'\d+', cols[idx_umaban].get_text(strip=True))
            if r_m and u_m: result_map[int(u_m.group())] = int(r_m.group())
            elif u_m: result_map[int(u_m.group())] = 99
        
        return result_map, info, "success"
    except Exception as e: return None, None, str(e)

# --- 4. UI ç”»é¢è¡¨ç¤º ---
st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¸€æ‹¬ç‰ˆï¼‰")

with st.sidebar:
    st.header("ğŸ“‚ 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    up_curr = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿(Excel/CSV)", type=['xlsx', 'csv'], key="curr")
    
    if 'analyzed_df' in st.session_state:
        st.divider()
        st.header("ğŸ’¾ 3. ä¿å­˜")
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ç€é †å…¥ã‚ŠCSVã‚’ä¿å­˜", csv, "horse_results.csv")
        if st.sidebar.button("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢"):
            del st.session_state['analyzed_df']; st.rerun()

if up_curr:
    df_raw, status = load_data(up_curr)
    if status == "success" and not df_raw.empty:
        if 'analyzed_df' not in st.session_state:
            st.session_state['analyzed_df'] = df_raw
        
        df_work = st.session_state['analyzed_df']

        # --- ä¸€æ‹¬å–å¾—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
        st.header("ğŸ”— 2. çµæœã®ä¸€æ‹¬å–å¾—")
        with st.expander("ã“ã“ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦URLã‚’ã¾ã¨ã‚ã¦è²¼ã‚Šä»˜ã‘", expanded=True):
            urls_input = st.text_area("ãƒãƒƒãƒˆç«¶é¦¬ã®çµæœURLï¼ˆ1è¡Œã«1ã¤ãšã¤è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ï¼‰", height=200, help="çµæœãƒšãƒ¼ã‚¸ã®URLã‚’ã¾ã¨ã‚ã¦ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆã—ã¦ãã ã•ã„")
            col1, col2 = st.columns([1, 4])
            with col1:
                bulk_btn = st.button("ğŸš€ ä¸€æ‹¬å–å¾—é–‹å§‹")
            with col2:
                st.caption("â€»å–å¾—ã«ã¯1ãƒ¬ãƒ¼ã‚¹æ•°ç§’ã‹ã‹ã‚Šã¾ã™ã€‚é€”ä¸­ã§ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ãªã„ã§ãã ã•ã„ã€‚")

            if bulk_btn and urls_input:
                urls = [u.strip() for u in urls_input.split('\n') if u.strip()]
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                success_count = 0
                for i, url in enumerate(urls):
                    status_text.text(f"å–å¾—ä¸­ ({i+1}/{len(urls)}): {url[:50]}...")
                    res, info, msg = fetch_netkeiba_result(url)
                    
                    if msg == "success" and info["place"]:
                        for u, r in res.items():
                            st.session_state['analyzed_df'].loc[
                                (st.session_state['analyzed_df']['å ´å'] == info["place"]) & 
                                (st.session_state['analyzed_df']['R'] == info["r"]) & 
                                (st.session_state['analyzed_df']['æ­£ç•ª'] == u), 'ç€é †'
                            ] = r
                        success_count += 1
                    else:
                        st.warning(f"ã‚¹ã‚­ãƒƒãƒ—: {info['place'] if info else ''}{info['r'] if info else ''}R ({msg})")
                    
                    # é€²è¡ŒçŠ¶æ³æ›´æ–°
                    progress_bar.progress((i + 1) / len(urls))
                    time.sleep(1.5) # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ
                
                status_text.success(f"å®Œäº†ï¼ {len(urls)}ä»¶ä¸­ {success_count}ä»¶ã®ãƒ¬ãƒ¼ã‚¹ã‚’åæ˜ ã—ã¾ã—ãŸã€‚")
                st.rerun()

        # --- å€‹åˆ¥ç¢ºèªãƒ»ä¿®æ­£ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
        st.divider()
        places = [p for p in df_work['å ´å'].unique().tolist() if str(p) != 'nan' and p != '']
        if places:
            st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªãƒ»å€‹åˆ¥ä¿®æ­£")
            p_tabs = st.tabs(places)
            for p_tab, place in zip(p_tabs, places):
                with p_tab:
                    p_df = df_work[df_work['å ´å'] == place]
                    r_nums = sorted([int(r) for r in p_df['R'].unique() if not pd.isna(r)])
                    r_num = st.selectbox(f"ãƒ¬ãƒ¼ã‚¹é¸æŠ ({place})", r_nums, key=f"sel_{place}")
                    
                    current_race = st.session_state['analyzed_df'][
                        (st.session_state['analyzed_df']['å ´å'] == place) & 
                        (st.session_state['analyzed_df']['R'] == r_num)
                    ].sort_values('æ­£ç•ª')
                    
                    edited = st.data_editor(
                        current_race[['æ­£ç•ª', 'é¦¬å', 'ç€é †', 'å˜ï½µï½¯ï½½ï¾', 'é¨æ‰‹']],
                        hide_index=True, use_container_width=True, key=f"ed_{place}_{r_num}"
                    )
                    
                    if st.button(f"âœ… {place}{r_num}R ã®å¤‰æ›´ã‚’ä¿å­˜", key=f"save_{place}_{r_num}"):
                        for _, row in edited.iterrows():
                            st.session_state['analyzed_df'].loc[
                                (st.session_state['analyzed_df']['å ´å'] == place) & 
                                (st.session_state['analyzed_df']['R'] == r_num) & 
                                (st.session_state['analyzed_df']['æ­£ç•ª'] == row['æ­£ç•ª']), 'ç€é †'
                            ] = row['ç€é †']
                        st.success("ä¿å­˜ã—ã¾ã—ãŸ")
    else:
        st.info("å·¦å´ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰å½“æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
