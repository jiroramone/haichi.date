import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px
import urllib.request
from bs4 import BeautifulSoup

# --- 1. åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

# åŠè§’å¤‰æ›ãƒ˜ãƒ«ãƒ‘ãƒ¼
def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    return re.sub(r'[^\d\.]', '', text.translate(table))

# åå‰æ­£è¦åŒ–
def normalize_name(x):
    if pd.isna(x): return ''
    s = str(x).strip().replace('ã€€', '').replace(' ', '')
    s = re.split(r'[,(ï¼ˆ/]', s)[0]
    return re.sub(r'[â˜…â˜†â–²â–³â—‡$*]', '', s)

# --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
@st.cache_data
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        if not any(col in str(df.columns) for col in ['é¦¬', 'ç•ª', 'R', 'é¨']):
            for i in range(min(len(df), 10)):
                if any(x in str(df.iloc[i].values) for x in ['é¦¬', 'ç•ª', 'R']):
                    df.columns = df.iloc[i]
                    df = df.iloc[i+1:].reset_index(drop=True)
                    break

        df.columns = df.columns.astype(str).str.strip()
        name_map = {
            'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 'ç«¶é¦¬å ´': 'å ´å',
            'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'å©èˆå': 'å©èˆ',
            'é¨æ‰‹å': 'é¨æ‰‹', 'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª',
            'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾',
            'æ­£å¾ª': 'æ­£å¾ªç’°', 'é€†å¾ª': 'é€†å¾ªç’°', 'ç€': 'ç€é †'
        }
        df = df.rename(columns=name_map)
        ensure_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'ç€é †']
        for col in ensure_cols:
            if col not in df.columns: df[col] = np.nan

        df['R'] = pd.to_numeric(df['R'].apply(to_half_width), errors='coerce')
        df['æ­£ç•ª'] = pd.to_numeric(df['æ­£ç•ª'].apply(to_half_width), errors='coerce')
        df = df.dropna(subset=['R', 'æ­£ç•ª'])
        df['R'] = df['R'].astype(int); df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)
        for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'é¦¬å', 'å ´å']:
            df[col] = df[col].apply(normalize_name)
        df['å˜ï½µï½¯ï½½ï¾'] = pd.to_numeric(df['å˜ï½µï½¯ï½½ï¾'].apply(to_half_width), errors='coerce')
        return df.copy(), "success"
    except Exception as e: return pd.DataFrame(), str(e)

# --- 3. é…ç½®è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ ---
def analyze_haichi(df_curr, df_prev=None):
    df = df_curr.copy()
    if 'ã‚¿ã‚¤ãƒ—' in df.columns and df['ã‚¿ã‚¤ãƒ—'].notna().any(): return df
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['é ­æ•°'] = max_umaban.fillna(16).astype(int)
    df['é€†ç•ª'] = (df['é ­æ•°'] + 1) - df['æ­£ç•ª']
    df['æ­£å¾ªç’°'] = df['é ­æ•°'] + df['æ­£ç•ª']
    df['é€†å¾ªç’°'] = df['é ­æ•°'] + df['é€†ç•ª']
    for c in ['æ­£ç•ª', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']:
        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0).astype(int)
    df['ã‚¿ã‚¤ãƒ—_list'] = [[] for _ in range(len(df))]
    df['å±æ€§_list'] = [[] for _ in range(len(df))]
    df['ãƒ‘ã‚¿ãƒ¼ãƒ³_list'] = [[] for _ in range(len(df))]
    df['ã‚¹ã‚³ã‚¢'] = 0.0
    idx_map = {(row['å ´å'], row['R'], row['æ­£ç•ª']): idx for idx, row in df.iterrows()}
    blue_info = []
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        g_keys = ['å ´å', col] if col == 'é¨æ‰‹' else [col]
        for name, group in df.groupby(g_keys):
            if len(group) < 2 or not name: continue
            all_sets = [{r['æ­£ç•ª'], r['é€†ç•ª'], r['æ­£å¾ªç’°'], r['é€†å¾ªç’°']} for _, r in group.iterrows()]
            common = set.intersection(*all_sets)
            if common:
                for _, row in group.iterrows():
                    idx = idx_map.get((row['å ´å'], row['R'], row['æ­£ç•ª']))
                    if idx is not None:
                        df.at[idx, 'ã‚¿ã‚¤ãƒ—_list'].append(f'â˜…{col}é’å¡—'); df.at[idx, 'å±æ€§_list'].append(f'{col}:{name}')
                        df.at[idx, 'ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].append('é’å¡—'); df.at[idx, 'ã‚¹ã‚³ã‚¢'] += 9.0 + (1.0 if col == 'é¨æ‰‹' else 0.2)
                        blue_info.append({'å ´å':row['å ´å'], 'R':row['R'], 'æ­£ç•ª':row['æ­£ç•ª'], 'å±æ€§':f"{col}:{name}"})
    for b in blue_info:
        for t_num in [b['æ­£ç•ª']-1, b['æ­£ç•ª']+1]:
            key = (b['å ´å'], b['R'], t_num)
            if key in idx_map:
                idx = idx_map[key]
                if not any('é’å¡—éš£' in str(x) for x in df.at[idx, 'ã‚¿ã‚¤ãƒ—_list']):
                    df.at[idx, 'ã‚¿ã‚¤ãƒ—_list'].append('â–³é’å¡—éš£'); df.at[idx, 'å±æ€§_list'].append(f'éš£:{b["å±æ€§"]}'); df.at[idx, 'ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].append('é’éš£'); df.at[idx, 'ã‚¹ã‚³ã‚¢'] += 9.0
    pair_labels = list("ABCDEFGHIJKLMNOP")
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        for name, group in df.groupby(['å ´å', col] if col=='é¨æ‰‹' else col):
            if len(group) < 2 or not name: continue
            rows = group.sort_values('R').to_dict('records')
            for i in range(len(rows)-1):
                r1, r2 = rows[i], rows[i+1]
                v1, v2 = [r1[c] for c in ['æ­£ç•ª','é€†ç•ª','æ­£å¾ªç’°','é€†å¾ªç’°']], [r2[c] for c in ['æ­£ç•ª','é€†ç•ª','æ­£å¾ªç’°','é€†å¾ªç’°']]
                pats = [pair_labels[x*4+y] for x in range(4) for y in range(4) if v1[x]==v2[y] and v1[x]!=0]
                if pats:
                    is_c = any(x in pats for x in ['C','D','G','H'])
                    for r_data in [r1, r2]:
                        idx = idx_map.get((r_data['å ´å'], r_data['R'], r_data['æ­£ç•ª']))
                        if idx is not None:
                            df.at[idx, 'ã‚¿ã‚¤ãƒ—_list'].append('â—ãƒãƒ£ãƒ³ã‚¹' if is_c else 'â—‹ç‹™ã„ç›®')
                            df.at[idx, 'å±æ€§_list'].append(f'{col}:{name}'); df.at[idx, 'ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].append("".join(pats))
                            df.at[idx, 'ã‚¹ã‚³ã‚¢'] += 4.0 if is_c else 3.0
    if df_prev is not None and not df_prev.empty:
        for idx, row in df.iterrows():
            prev_match = df_prev[(df_prev['å ´å'] == row['å ´å']) & (df_prev['R'] == row['R']) & (df_prev['é¨æ‰‹'] == row['é¨æ‰‹'])]
            for _, p_row in prev_match.iterrows():
                if {row['æ­£ç•ª'],row['é€†ç•ª'],row['æ­£å¾ªç’°'],row['é€†å¾ªç’°']}.intersection({p_row['æ­£ç•ª'],p_row['é€†ç•ª'],p_row['æ­£å¾ªç’°'],p_row['é€†å¾ªç’°']}):
                    df.at[idx, 'ã‚¿ã‚¤ãƒ—_list'].append('â˜…å‰æ—¥åŒé…ç½®'); df.at[idx, 'å±æ€§_list'].append(f'å‰æ—¥:é¨æ‰‹:{row["é¨æ‰‹"]}'); df.at[idx, 'ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].append('å‰æ—¥'); df.at[idx, 'ã‚¹ã‚³ã‚¢'] += 8.3
    df['ã‚¿ã‚¤ãƒ—'] = df['ã‚¿ã‚¤ãƒ—_list'].apply(lambda x: ' / '.join(x) if isinstance(x, list) else x)
    df['å±æ€§'] = df['å±æ€§_list'].apply(lambda x: ' / '.join(list(set(x))) if isinstance(x, list) else x)
    df['ãƒ‘ã‚¿ãƒ¼ãƒ³'] = df['ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)
    return df

# --- 4. åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ ---
def apply_ranking_logic(df_in):
    if df_in.empty: return df_in
    df = df_in.copy()
    df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
    hit_results = df[df['ç€é †'] <= 3]
    hit_attrs = set([a.replace('éš£:', '').replace('å‰æ—¥:', '') for _, row in hit_results.iterrows() for a in str(row.get('å±æ€§', '')).split(' / ')])
    hit_pats = set([p for pats in hit_results['ãƒ‘ã‚¿ãƒ¼ãƒ³'].dropna() for p in str(pats).split(',') if p])
    def get_metrics(row):
        score = row.get('ã‚¹ã‚³ã‚¢', 0); p_list = str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')).split(',')
        bonus = 4.0 if any(p in hit_pats and len(p)==1 for p in p_list) else 0.0
        reasons = []
        for ra in str(row.get('å±æ€§', '')).split(' / '):
            is_neighbor = ra.startswith('éš£:'); cra = ra.replace('éš£:', '').replace('å‰æ—¥:', '')
            if cra in hit_attrs: reasons.append("æœ¬ä½“å¥½èµ°" if is_neighbor else f"{cra.split(':')[0] if ':' in cra else 'æœ¬äºº'}å¥½èµ°")
        penalty = -3.0 if reasons else 0.0
        total = score + bonus + penalty + (-30.0 if pd.to_numeric(row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce') > 49.9 else 0.0)
        rec = "ğŸ‘‘ ç›¤çŸ³ã®è»¸" if total >= 15 else "âœ¨ æ¨å¥¨è»¸" if total >= 12 else "ğŸ”¥ æ¿€ç†±ç›¸æ‰‹" if total >= 10 else "â–² é…ç½®æ³¨ç›®" if score > 0 else ""
        return pd.Series([total, f"âš ï¸{','.join(set(reasons))}(-3)" if reasons else "", rec])
    df[['ç·åˆã‚¹ã‚³ã‚¢', 'ã‚¨ãƒãƒ«ã‚®ãƒ¼çŠ¶æ…‹', 'æ¨å¥¨è²·ã„ç›®']] = df.apply(get_metrics, axis=1)
    return df

# --- 5. ãƒãƒƒãƒˆç«¶é¦¬è‡ªå‹•å–å¾— (BeautifulSoupç‰ˆ) ---
def fetch_netkeiba_result(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req) as response:
            html = response.read().decode('euc-jp', errors='replace')
        
        soup = BeautifulSoup(html, 'html.parser')
        table = soup.find('table', id='All_Result_Table')
        
        if not table:
            return None, "ç€é †ãƒ†ãƒ¼ãƒ–ãƒ«(All_Result_Table)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"

        result_map = {}
        rows = table.find_all('tr', class_='HorseList')
        
        for row in rows:
            cols = row.find_all('td')
            if len(cols) < 3: continue
            try:
                rank_text = cols[0].get_text(strip=True)
                umaban_text = cols[2].get_text(strip=True)
                rank_match = re.search(r'\d+', rank_text)
                umaban_match = re.search(r'\d+', umaban_text)
                if rank_match and umaban_match:
                    result_map[int(umaban_match.group())] = int(rank_match.group())
                elif umaban_match:
                    result_map[int(umaban_match.group())] = 99 # æ•°å­—ä»¥å¤–ã¯ç€å¤–æ‰±ã„
            except: continue
                
        return result_map, "success"
    except Exception as e:
        return None, str(e)

# --- 6. UI ---
st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ‡ãƒ¼ã‚¿åé›†ç”¨ï¼‰")
with st.sidebar:
    st.header("ğŸ“‚ èª­ã¿è¾¼ã¿")
    up_curr = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿", type=['xlsx', 'csv'], key="curr")
    up_prev = st.file_uploader("å‰æ—¥ãƒ‡ãƒ¼ã‚¿", type=['xlsx', 'csv'], key="prev")
    if up_curr:
        pure_name = up_curr.name.replace('progress_', '').replace('.csv', '').replace('.xlsx', '')
        if "current_pure_name" not in st.session_state: st.session_state["current_pure_name"] = pure_name
        elif st.session_state["current_pure_name"] != pure_name:
            st.session_state["current_pure_name"] = pure_name
            if "analyzed_df" in st.session_state: del st.session_state["analyzed_df"]; st.rerun()
    st.divider()
    if 'analyzed_df' in st.session_state:
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ç€é †å…¥ã‚ŠCSVã‚’ä¿å­˜", csv, f"progress_{up_curr.name if up_curr else 'data'}.csv")

if up_curr:
    df_raw, status = load_data(up_curr)
    df_p_raw, _ = load_data(up_prev) if up_prev else (None, None)
    if status == "success":
        if 'analyzed_df' not in st.session_state: st.session_state['analyzed_df'] = apply_ranking_logic(analyze_haichi(df_raw, df_p_raw))
        
        # å†…éƒ¨ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ç”¨ã«é–¢æ•°åŒ–
        def update_ranks(fetched_ranks):
            df = st.session_state['analyzed_df'].copy()
            for u, r in fetched_ranks.items():
                # å ´åã¨Rã€æ­£ç•ªãŒä¸€è‡´ã™ã‚‹è¡Œã‚’æ›´æ–°
                # (URLãŒ1ãƒ¬ãƒ¼ã‚¹ã”ã¨ãªã®ã§ã€å…¨ãƒ¬ãƒ¼ã‚¹ä¸€æ‹¬æ›´æ–°ã¯UIå´ã®ãƒ«ãƒ¼ãƒ—ã§å‡¦ç†)
                pass

        full_df = st.session_state['analyzed_df']
        st.subheader("ğŸ“ çµæœå…¥åŠ›")
        
        # çµæœå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
        with st.form("result_form"):
            places = sorted(full_df['å ´å'].unique())
            p_tabs = st.tabs(places); edited_dfs = []
            for p_tab, place in zip(p_tabs, places):
                with p_tab:
                    p_df = full_df[full_df['å ´å'] == place]
                    r_nums = sorted(p_df['R'].unique())
                    r_tabs = st.tabs([f"{r}R" for r in r_nums])
                    for r_tab, r_num in zip(r_tabs, r_nums):
                        with r_tab:
                            race_full = p_df[p_df['R'] == r_num].sort_values('æ­£ç•ª')
                            
                            # è‡ªå‹•å–å¾—ã‚¨ãƒªã‚¢
                            c1, c2 = st.columns([3, 1])
                            with c1: nk_url = st.text_input(f"ãƒãƒƒãƒˆç«¶é¦¬URL ({place}{r_num}R)", key=f"url_{place}_{r_num}")
                            with c2: 
                                # å„ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®è‡ªå‹•å–å¾—ãƒœã‚¿ãƒ³
                                auto_btn = st.form_submit_button(f"ğŸŒ è‡ªå‹•å–å¾—", key=f"btn_{place}_{r_num}")
                            
                            if auto_btn and nk_url:
                                res, msg = fetch_netkeiba_result(nk_url)
                                if msg == "success":
                                    st.success(f"{len(res)}é ­ã®ç€é †ã‚’å–å¾—ã—ã¾ã—ãŸï¼")
                                    for u, r in res.items():
                                        race_full.loc[race_full['æ­£ç•ª'] == u, 'ç€é †'] = r
                                else: st.error(f"å–å¾—å¤±æ•—: {msg}")
                            
                            # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºãƒ»ç·¨é›†
                            ed = st.data_editor(
                                race_full[['æ­£ç•ª','é¦¬å','ç€é †','å˜ï½µï½¯ï½½ï¾','å±æ€§','ã‚¨ãƒãƒ«ã‚®ãƒ¼çŠ¶æ…‹','ç·åˆã‚¹ã‚³ã‚¢']], 
                                hide_index=True, use_container_width=True, key=f"ed_{place}_{r_num}"
                            )
                            # ç·¨é›†çµæœã‚’åé›†
                            updated = race_full.copy()
                            for _, row in ed.iterrows():
                                updated.loc[updated['æ­£ç•ª'] == row['æ­£ç•ª'], 'ç€é †'] = row['ç€é †']
                            edited_dfs.append(updated)
            
            # å…¨ä½“ç¢ºå®šãƒœã‚¿ãƒ³
            if st.form_submit_button("ğŸ”„ å…¥åŠ›ã‚’ç¢ºå®šã—ã¦å…¨ä½“ã‚’æ›´æ–°"):
                st.session_state['analyzed_df'] = apply_ranking_logic(pd.concat(edited_dfs, ignore_index=True))
                st.rerun()

        # æ¨å¥¨é¦¬è¡¨ç¤º
        st.divider(); st.subheader("ğŸ‘‘ ç‰¹é¸æ¨å¥¨é¦¬")
        future_df = full_df[(full_df['ç€é †'].isna()) & (full_df['ç·åˆã‚¹ã‚³ã‚¢'] >= 10)]
        if not future_df.empty:
            for pl in sorted(future_df['å ´å'].unique()):
                st.write(f"### {pl}")
                st.dataframe(future_df[future_df['å ´å'] == pl][['R','æ­£ç•ª','é¦¬å','å˜ï½µï½¯ï½½ï¾','å±æ€§','ã‚¨ãƒãƒ«ã‚®ãƒ¼çŠ¶æ…‹','ç·åˆã‚¹ã‚³ã‚¢']], use_container_width=True, hide_index=True)
