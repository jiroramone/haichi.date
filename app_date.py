import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from bs4 import BeautifulSoup
import time

st.set_page_config(page_title="ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰", layout="wide")

# --- 1. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def to_half_width(text):
    if pd.isna(text): return text
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    return re.sub(r'[^\d\.]', '', str(text).translate(table))

def normalize_name(x):
    if pd.isna(x): return ''
    s = str(x).strip().replace('ã€€', '').replace(' ', '')
    return re.split(r'[,(ï¼ˆ/]', s)[0]

JYO_MAP = {'01':'æœ­å¹Œ','02':'å‡½é¤¨','03':'ç¦å³¶','04':'æ–°æ½Ÿ','05':'æ±äº¬','06':'ä¸­å±±','07':'ä¸­äº¬','08':'äº¬éƒ½','09':'é˜ªç¥','10':'å°å€‰'}

# --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆé‡è¤‡å›é¿æ©Ÿèƒ½ä»˜ãï¼‰ ---
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        # 1. èª­ã¿è¾¼ã¿ç›´å¾Œã®åˆ—åã®é‡è¤‡ã‚’å¼·åˆ¶å›é¿
        cols = pd.Series(df.columns)
        for d in cols[cols.duplicated()].unique():
            cols[cols == d] = [f"{d}_{i}" if i != 0 else d for i in range(len(cols[cols == d]))]
        df.columns = cols

        # 2. é …ç›®åã‚’æ¢ã™ï¼ˆ20è¡Œç›®ã¾ã§ã‚¹ã‚­ãƒ£ãƒ³ï¼‰
        for i in range(min(len(df), 20)):
            row_vals = [str(x) for x in df.iloc[i].values]
            if any('å ´æ‰€' in x or 'R' in x or 'é¦¬å' in x for x in row_vals):
                df.columns = df.iloc[i]
                df = df.iloc[i+1:].reset_index(drop=True)
                break
        
        # 3. åˆ—åã®æ­£è¦åŒ–ï¼ˆã“ã“ã§ã‚‚é‡è¤‡ãŒèµ·ããªã„ã‚ˆã†ã«åˆ¶å¾¡ï¼‰
        df.columns = [str(c).strip() for c in df.columns]
        name_map = {'å ´æ‰€':'å ´å','R':'R','ï¼²':'R','ç•ª':'æ­£ç•ª','é¦¬ç•ª':'æ­£ç•ª','ç€é †':'ç€é †','ç€':'ç€é †','å˜å‹ã‚ªãƒƒã‚º':'å˜ï½µï½¯ï½½ï¾','ã‚ªãƒƒã‚º':'å˜ï½µï½¯ï½½ï¾'}
        
        new_columns = []
        used_names = set()
        for c in df.columns:
            target_name = c
            for k, v in name_map.items():
                if k == c: # å®Œå…¨ä¸€è‡´ã‚’å„ªå…ˆ
                    target_name = v
                    break
            
            # ã‚‚ã—æ›¸ãæ›ãˆå¾Œã®åå‰ãŒæ—¢ã«ä½¿ã‚ã‚Œã¦ã„ãŸã‚‰ç•ªå·ã‚’ã¤ã‘ã‚‹
            base_name = target_name
            counter = 1
            while target_name in used_names:
                target_name = f"{base_name}_{counter}"
                counter += 1
            
            new_columns.append(target_name)
            used_names.add(target_name)
        
        df.columns = new_columns

        # æœ€ä½é™ã®åˆ—ã‚’ç¢ºä¿
        for col in ['å ´å', 'R', 'æ­£ç•ª', 'ç€é †']:
            if col not in df.columns: df[col] = np.nan
        
        return df, "success"
    except Exception as e:
        return pd.DataFrame(), str(e)

# --- 3. ãƒãƒƒãƒˆç«¶é¦¬å–å¾— ---
def fetch_netkeiba_result(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'EUC-JP'
        
        rid_match = re.search(r'race_id=(\d{12})', url)
        info = {"place": JYO_MAP.get(rid_match.group(1)[4:6], "") if rid_match else "", "r": int(rid_match.group(1)[10:12]) if rid_match else 0}

        soup = BeautifulSoup(response.text, 'html.parser')
        table = soup.find('table', id='All_Result_Table') or soup.find('table', class_=lambda x: x and 'ResultRefund' in x)
        if not table: return None, info, "è¡¨ãªã—"

        result_map = {}
        rows = table.find_all('tr', class_=lambda x: x and 'HorseList' in x)
        for row in rows:
            cols = row.find_all('td')
            if len(cols) < 3: continue
            r_m = re.search(r'\d+', cols[0].get_text(strip=True))
            u_m = re.search(r'\d+', cols[2].get_text(strip=True))
            if u_m: result_map[int(u_m.group())] = int(r_m.group()) if r_m else 99
        return result_map, info, "success"
    except Exception as e: return None, None, str(e)

# --- 4. UI ç”»é¢ ---
st.title("ğŸ‡ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé‡è¤‡ã‚¨ãƒ©ãƒ¼å¯¾ç­–ç‰ˆï¼‰")

up_curr = st.sidebar.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=['xlsx', 'csv'])

if up_curr:
    if 'df' not in st.session_state:
        df, status = load_data(up_curr)
        st.session_state['df'] = df

    st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # URLä¸€æ‹¬è²¼ã‚Šä»˜ã‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.header("ğŸ”— URLä¸€æ‹¬è²¼ã‚Šä»˜ã‘")
    urls_input = st.text_area("ãƒãƒƒãƒˆç«¶é¦¬ã®çµæœURLã‚’1è¡Œãšã¤è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„", height=200)
    
    if st.button("ğŸš€ ä¸€æ‹¬å–å¾—é–‹å§‹"):
        if urls_input:
            urls = [u.strip() for u in urls_input.split('\n') if u.strip()]
            progress = st.progress(0)
            for i, url in enumerate(urls):
                res, info, msg = fetch_netkeiba_result(url)
                if msg == "success":
                    for u, r in res.items():
                        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®šã—ã¦ç€é †ã‚’æ›´æ–°
                        st.session_state['df'].loc[(st.session_state['df']['å ´å']==info['place']) & (st.session_state['df']['R']==info['r']) & (st.session_state['df']['æ­£ç•ª']==u), 'ç€é †'] = r
                    st.write(f"âœ… å–å¾—æˆåŠŸ: {info['place']}{info['r']}R")
                else:
                    st.error(f"âŒ å¤±æ•—: {url[-12:]} ({msg})")
                progress.progress((i+1)/len(urls))
                time.sleep(1)
            st.rerun()

    st.divider()
    st.subheader("ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    # é‡è¤‡å›é¿ã—ãŸdfã‚’è¡¨ç¤º
    st.dataframe(st.session_state['df'], use_container_width=True)
    
    csv = st.session_state['df'].to_csv(index=False).encode('utf-8-sig')
    st.sidebar.download_button("ğŸ“¥ ç€é †å…¥ã‚ŠCSVã‚’ä¿å­˜", csv, "horse_results.csv")
    if st.sidebar.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢"):
        del st.session_state['df']; st.rerun()

else:
    st.info("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
