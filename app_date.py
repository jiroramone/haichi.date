import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from bs4 import BeautifulSoup
import urllib.request

# --- 1. åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    return re.sub(r'[^\d\.]', '', text.translate(table))

def normalize_name(x):
    if pd.isna(x): return ''
    s = str(x).strip().replace('ã€€', '').replace(' ', '')
    s = re.split(r'[,(ï¼ˆ/]', s)[0]
    return re.sub(r'[â˜…â˜†â–²â–³â—‡$*]', '', s)

# --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã€Œå ´æ‰€ã€åˆ—ã«ç‰¹åŒ–ï¼‰ ---
@st.cache_data
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½ç½®ã®è‡ªå‹•èª¿æ•´ï¼ˆ10è¡Œç›®ã¾ã§ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦é …ç›®åã‚’æ¢ã™ï¼‰
        found_header = False
        if not any(col in str(df.columns) for col in ['å ´æ‰€', 'é¦¬', 'ç•ª', 'R']):
            for i in range(min(len(df), 10)):
                row_values = [str(x) for x in df.iloc[i].values]
                if any('å ´æ‰€' in x or 'ç•ª' in x or 'R' in x for x in row_values):
                    df.columns = df.iloc[i]
                    df = df.iloc[i+1:].reset_index(drop=True)
                    found_header = True
                    break

        df.columns = df.columns.astype(str).str.strip()
        
        # åˆ—åã®åå¯„ã›ï¼ˆã€Œå ´æ‰€ã€ã‚’ã€Œå ´åã€ã¨ã—ã¦å†…éƒ¨çµ±ä¸€ï¼‰
        name_map = {
            'å ´æ‰€': 'å ´å', 'ç«¶é¦¬å ´': 'å ´å', 'é–‹å‚¬': 'å ´å',
            'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª',
            'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾',
            'ç€': 'ç€é †'
        }
        df = df.rename(columns=name_map)
        
        # å¿…é ˆã‚«ãƒ©ãƒ ã®ãƒã‚§ãƒƒã‚¯ã¨ä½œæˆ
        ensure_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'ç€é †']
        for col in ensure_cols:
            if col not in df.columns:
                df[col] = np.nan

        # å‹å¤‰æ›
        df['R'] = pd.to_numeric(df['R'].apply(to_half_width), errors='coerce')
        df['æ­£ç•ª'] = pd.to_numeric(df['æ­£ç•ª'].apply(to_half_width), errors='coerce')
        df = df.dropna(subset=['R', 'æ­£ç•ª'])
        df['R'] = df['R'].astype(int); df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)
        
        for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'é¦¬å', 'å ´å']:
            df[col] = df[col].astype(str).apply(normalize_name)
            
        df['å˜ï½µï½¯ï½½ï¾'] = pd.to_numeric(df['å˜ï½µï½¯ï½½ï¾'].apply(to_half_width), errors='coerce')
        return df.copy(), "success"
    except Exception as e:
        return pd.DataFrame(), str(e)

# --- 3. ãƒãƒƒãƒˆç«¶é¦¬è‡ªå‹•å–å¾— ---
def fetch_netkeiba_result(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
            'Referer': 'https://race.netkeiba.com/'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'EUC-JP'
        if response.status_code != 200: return None, f"ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦(Error {response.status_code})"

        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.find_all('table')
        target_table = None
        for t in tables:
            t_text = t.get_text()
            if 'ç€é †' in t_text and 'é¦¬ç•ª' in t_text:
                target_table = t
                break
        
        if not target_table: return None, "ç€é †è¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

        result_map = {}
        rows = target_table.find_all('tr')
        header_cols = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]
        
        try:
            idx_rank = [i for i, c in enumerate(header_cols) if 'ç€é †' in c][0]
            idx_umaban = [i for i, c in enumerate(header_cols) if 'é¦¬ç•ª' in c][0]
        except: return None, "åˆ—ã®ç‰¹å®šã«å¤±æ•—ã—ã¾ã—ãŸ"

        for row in rows[1:]:
            cols = row.find_all('td')
            if len(cols) <= max(idx_rank, idx_umaban): continue
            r_txt = cols[idx_rank].get_text(strip=True)
            u_txt = cols[idx_umaban].get_text(strip=True)
            r_m = re.search(r'\d+', r_txt)
            u_m = re.search(r'\d+', u_txt)
            if r_m and u_m: result_map[int(u_m.group())] = int(r_m.group())
            elif u_m: result_map[int(u_m.group())] = 99
        
        return result_map, "success"
    except Exception as e: return None, str(e)

# --- 4. UI ç”»é¢è¡¨ç¤º ---
st.title("ğŸ‡ é…ç½®é¦¬åˆ¸ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ")

with st.sidebar:
    st.header("ğŸ“‚ èª­ã¿è¾¼ã¿")
    up_curr = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿(Excel/CSV)", type=['xlsx', 'csv'], key="curr")
    
    if 'analyzed_df' in st.session_state:
        st.divider()
        st.header("ğŸ’¾ ä¿å­˜")
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ç€é †å…¥ã‚ŠCSVã‚’ä¿å­˜", csv, "horse_data_with_results.csv")
        if st.button("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ"):
            del st.session_state['analyzed_df']
            st.rerun()

if up_curr:
    df_raw, status = load_data(up_curr)
    
    if status == "success" and not df_raw.empty:
        # åˆå›èª­ã¿è¾¼ã¿æ™‚ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ç™»éŒ²
        if 'analyzed_df' not in st.session_state:
            st.session_state['analyzed_df'] = df_raw
        
        df_work = st.session_state['analyzed_df']
        
        # ä¼šå ´ï¼ˆå ´æ‰€ï¼‰ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        places = [p for p in df_work['å ´å'].unique().tolist() if str(p) != 'nan' and p != '']
        
        if places:
            st.subheader("ğŸ“ ãƒ¬ãƒ¼ã‚¹çµæœã®è‡ªå‹•å–å¾—")
            p_tabs = st.tabs(places)
            for p_tab, place in zip(p_tabs, places):
                with p_tab:
                    p_df = df_work[df_work['å ´å'] == place]
                    r_nums = sorted([int(r) for r in p_df['R'].unique() if not pd.isna(r)])
                    
                    if r_nums:
                        r_num = st.selectbox(f"ãƒ¬ãƒ¼ã‚¹ã‚’é¸æŠ ({place})", r_nums, key=f"sel_{place}")
                        
                        with st.form(key=f"form_{place}_{r_num}"):
                            url = st.text_input("ãƒãƒƒãƒˆç«¶é¦¬çµæœURL", placeholder="https://race.netkeiba.com/race/result.html?race_id=...")
                            btn = st.form_submit_button("ğŸŒ çµæœã‚’è‡ªå‹•å–å¾—")
                            
                            if btn and url:
                                res, msg = fetch_netkeiba_result(url)
                                if msg == "success":
                                    st.success(f"{len(res)}é ­ã®çµæœã‚’å–å¾—ã—ã¾ã—ãŸï¼")
                                    for u, r in res.items():
                                        st.session_state['analyzed_df'].loc[
                                            (st.session_state['analyzed_df']['å ´å'] == place) & 
                                            (st.session_state['analyzed_df']['R'] == r_num) & 
                                            (st.session_state['analyzed_df']['æ­£ç•ª'] == u), 'ç€é †'
                                        ] = r
                                else:
                                    st.error(f"å–å¾—å¤±æ•—: {msg}")
                        
                        st.write(f"ğŸ“Š {place}{r_num}R ã®ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿:")
                        current_race_data = st.session_state['analyzed_df'][
                            (st.session_state['analyzed_df']['å ´å'] == place) & 
                            (st.session_state['analyzed_df']['R'] == r_num)
                        ].sort_values('æ­£ç•ª')
                        
                        edited_data = st.data_editor(
                            current_race_data[['æ­£ç•ª', 'é¦¬å', 'ç€é †', 'å˜ï½µï½¯ï½½ï¾']],
                            hide_index=True, use_container_width=True, key=f"ed_{place}_{r_num}"
                        )
                        
                        if st.button(f"âœ… {place}{r_num}R ã®æ‰‹å‹•å…¥åŠ›ã‚’ä¿å­˜", key=f"save_{place}_{r_num}"):
                            for _, row in edited_data.iterrows():
                                st.session_state['analyzed_df'].loc[
                                    (st.session_state['analyzed_df']['å ´å'] == place) & 
                                    (st.session_state['analyzed_df']['R'] == r_num) & 
                                    (st.session_state['analyzed_df']['æ­£ç•ª'] == row['æ­£ç•ª']), 'ç€é †'
                                ] = row['ç€é †']
                            st.rerun()
            
            st.divider()
            st.info("ğŸ’¡ å–å¾—ãŒçµ‚ã‚ã£ãŸã‚‰ã€å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã€ŒğŸ“¥ ç€é †å…¥ã‚ŠCSVã‚’ä¿å­˜ã€ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

        else:
            st.error("âŒ ä¼šå ´åï¼ˆå ´æ‰€ï¼‰ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.write("ãƒ•ã‚¡ã‚¤ãƒ«å†…ã«ã€Œå ´æ‰€ã€ã¨ã„ã†åˆ—ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.write("èª­ã¿è¾¼ã‚“ã åˆ—å:", df_raw.columns.tolist())
            st.dataframe(df_raw.head())
    else:
        if up_curr:
            st.error(f"èª­ã¿è¾¼ã¿å¤±æ•—: {status}")
